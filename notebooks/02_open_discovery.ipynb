{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0110e8d-8665-4b98-b6a6-19feea38174c",
   "metadata": {},
   "source": [
    "# Open discovery model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7719a9-d567-437d-9875-37e6b2ba0275",
   "metadata": {},
   "source": [
    "## Motivation\n",
    "\n",
    "The main idea behind Swanson's work is that the two originally unconnected parts of the literature (C and A) can establish latent connections (through intermediate terms in B). These two pieces of literature have one or more b-terms in common, creating an indirect connection. Once these bridges are identified and validated, new knowledge can be generated.\n",
    "\n",
    "Swanson founded his discovery on the so-called ABC model, which is now widely regarded as a typical paradigm for LBD. The ABC model contains three concepts: the start concept ($c$), the intermediate concept ($b$), and the target concept ($a$). The LBD process begins with retrieving $c-b$ and $b-a$ relationships. Next, it combines associations with the same intermediates. Finally, it retrieves a list of $a-c$ relationships. If there is no prior mention of a particular $a-c$ connection, a hypothesis of a potential novel relationship between $a$ and $c$ concepts is conceived.\n",
    "\n",
    "<img src=\"img/open_discovery.png\" alt=\"Open discovery model\" width=\"300px\"/>\n",
    "\n",
    "In Swanson's scenario from 1988, literature domain $C$ refers to migraine (i.e., a set of papers mentioning migraine), whereas domain $A$ represents the fish oil literature (set of papers mentioning fish oil).\n",
    "\n",
    "Shortly after his seminal paper on ``undiscovered public knowledge'', \\citet{swanson1988migraine}, following the same discovery procedure, reported observations on the role of magnesium in migraine disorder. Magnesium insufficiency may exacerbate migraine due to complications involving stress (in connection with Type A personality), spreading cortical depression, epilepsy, platelet aggregation, serotonin, substance P, inflammation, vasoconstriction, prostaglandin formation, and hypoxia. Conversely, the ability of magnesium to block calcium channels may help to prevent migraine episodes.\n",
    "\n",
    "Using the Entrez query\n",
    "\\begin{minted}[breaklines]{text}\n",
    "migraine[TIAB] AND 1900/01/31:1987/12/31 [PDAT] AND medline [SB] AND english [LA]\n",
    "\\end{minted}\n",
    "we retrieved 3,058 bibliographic records from PubMed (as of May 17, 2024). The search strategy included four components. First, we manually selected the term \"magnesium\" to retrieve all relevant papers containing a source term in the title and/or abstract fields (\\texttt{TIAB}). Second, we applied a date range constraint to match the same period as in Swanson's work (\\texttt{PDAT}). Third, only high-quality articles published in the MEDLINE subset of PubMed were retrieved (\\texttt{SB}). The fourth component included articles written only in English (\\texttt{LA}).\n",
    "\n",
    "To map free text from the collected titles and abstracts to the UMLS concepts, we used the MetaMap tool.\n",
    "\n",
    "To reduce the search space during the LBD process, we can filter the UMLS-mapped concepts using semantic types. Semantic filters are query dependent. For instance, in the stage of selecting bridging $b$-concepts, we might select only those concepts that have a functional semantic type, such as ``Biologic Function'', ``Cell Function'', and ``Phenomenon or Process''.\n",
    "\n",
    "The open discovery setting is illustrated in Figure~\\ref{fig-OpenDiscovery}. Open discovery is in essence the approach employed for generating new hypotheses. Let us describe it in terms of Swanson's $ABC$ \\emph{model} \\citep{swanson1986undiscovered}. Let the starting point of research be term $c$, let $C$ be the literature about $c$, and let us assume that domain $A$ consists of the literature that might contain some---yet unknown---connections to $C$. The goal is to find $A$. This is done \n",
    "\\begin{quote}\n",
    "via some intermediate literature ($B$) toward an unknown destination $A$\\ldots  Success depends entirely on the knowledge and ingenuity of the searcher\\ldots \\citep{swanson1990medical}\n",
    "\\end{quote}\n",
    "\n",
    "!pip install scikit-learn\n",
    "!pip install tabulate\n",
    "\n",
    "# Load required libraries for NLP and data analysis\n",
    "import gzip\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "# import spacy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Read migraine Medline data with MeSH headings\n",
    "df_3 = pd.read_csv('./mig_mg/pmid_dp_ti_mh_20241124.psv.gz', sep='|', names=['pmid','dp','ti','mh_lst'])\n",
    "df_3.head()\n",
    "\n",
    "df_4 = pd.DataFrame(df_3['mh_lst'].str.split(';').tolist(), index=df_3.pmid).stack().reset_index([0, 'pmid'])\n",
    "df_4.columns = ['pmid', 'mh']\n",
    "df_4\n",
    "\n",
    "###########################\n",
    "# Parse a Descriptor Record\n",
    "\n",
    "import os\n",
    "# import pandas as pd\n",
    "from collections import defaultdict\n",
    "from itertools import groupby, filterfalse\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "# https://www.nlm.nih.gov/research/umls/knowledge_sources/semantic_network/SemGroups.txt\n",
    "def get_semantic_types(category=None):\n",
    "    # read in semantic types\n",
    "    st_df = pd.read_csv(\"https://semanticnetwork.nlm.nih.gov/download/SemGroups.txt\", delimiter=\"|\",\n",
    "                        names=[\"x0\", \"x1\", \"x2\", \"x3\"])\n",
    "    st_df = pd.read_csv(\"https://www.nlm.nih.gov/research/umls/knowledge_sources/semantic_network/SemGroups.txt\", delimiter=\"|\",\n",
    "                        names=[\"x0\", \"x1\", \"x2\", \"x3\"])\n",
    "    #st_d = dict(zip(st_df['x2'], st_df['x3']))\n",
    "    # cat_df = st_df.query(\"x1 == @category\")[['x2', 'x3']]\n",
    "    # semantic_types = set(cat_df['x2'])\n",
    "    # disorders = {'T037', 'T049', 'T048', 'T050', 'T184', 'T019', 'T190', 'T020', 'T033', 'T046', 'T191', 'T047'}\n",
    "    semantic_types = st_df\n",
    "    return semantic_types\n",
    "\n",
    "def parse_descriptor_records(mesh_desc_path, semantic_types = None):\n",
    "    \"\"\"\n",
    "    semantic_types is optional\n",
    "\n",
    "    :param mesh_desc_path:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    #TODO: only care about disease related attributes. !!!\n",
    "    # for example, ignoring RN CAS REGISTRY/EC NUMBER/UNII CODE  !!!\n",
    "\n",
    "    attributes = {'MH': \"term\",\n",
    "                  'MN': \"tree\",\n",
    "                  'FX': \"see_also\",\n",
    "                  'ST': \"semantic_type\",  # see: https://semanticnetwork.nlm.nih.gov/download/SemGroups.txt\n",
    "                  'MS': \"note\",\n",
    "                  'MR': \"last_updated\",\n",
    "                  'DC': \"descriptor_class\",\n",
    "                  'UI': \"_id\",\n",
    "                  'RECTYPE': \"record_type\",\n",
    "                  'synonyms': \"synonyms\"}  # added by me from PRINT ENTRY & ENTRY\n",
    "\n",
    "    # TODO: parse PRINT ENTRY and ENTRY completely\n",
    "    # \"'D-2-hydroxyglutaric aciduria|T047|EQV|OMIM (2013)|ORD (2010)|090615|abdeef'\"\n",
    "\n",
    "\n",
    "    # read in the mesh data\n",
    "    with open(mesh_desc_path) as f:\n",
    "        mesh_desc = [x.strip() for x in f.readlines()]\n",
    "\n",
    "    # which attributes can have multiple values?\n",
    "    gb = filterfalse(lambda x: x[0], groupby(mesh_desc, lambda x: x == \"*NEWRECORD\"))\n",
    "    ds = []\n",
    "    for gb_record in gb:\n",
    "        record = list(gb_record[1])\n",
    "        d = dict(Counter([line.split(\"=\", 1)[0].strip() for line in record if \"=\" in line]))\n",
    "        ds.append(d)\n",
    "    df = pd.DataFrame(ds).fillna(0)\n",
    "    list_attribs = set(df.columns[df.max() > 1])\n",
    "    # list_attribs = {'EC', 'ENTRY', 'FX', 'MH_TH', 'MN', 'PA', 'PI', 'PRINT ENTRY', 'RR', 'ST'}\n",
    "\n",
    "    # split into records\n",
    "    gb = filterfalse(lambda x: x[0], groupby(mesh_desc, lambda x: x == \"*NEWRECORD\"))\n",
    "\n",
    "    mesh_terms = dict()\n",
    "    for gb_record in gb:\n",
    "        record = list(gb_record[1])\n",
    "        d = defaultdict(list)\n",
    "        for line in record:\n",
    "            if \"=\" not in line:\n",
    "                continue\n",
    "            key = line.split(\"=\", 1)[0].strip()\n",
    "            value = line.split(\"=\", 1)[1].strip()\n",
    "            if key not in attributes and key not in list_attribs:\n",
    "                d[key] = value\n",
    "            elif key in list_attribs and key in attributes:\n",
    "                d[attributes[key]].append(value)\n",
    "            elif key in attributes and key not in {\"PRINT ENTRY\", \"ENTRY\"}:\n",
    "                d[attributes[key]] = value\n",
    "            elif key in {\"PRINT ENTRY\", \"ENTRY\"}:\n",
    "                d['synonyms'].append(value.split(\"|\", 1)[0])\n",
    "\n",
    "        if semantic_types and not (set(d['semantic_type']) & semantic_types):\n",
    "            continue\n",
    "        mesh_terms[d['_id']] = dict(d)\n",
    "\n",
    "    return mesh_terms\n",
    "###########################\n",
    "\n",
    "sty_df = get_semantic_types()\n",
    "\n",
    "sty_df.head()\n",
    "# print(sty_df.to_markdown())\n",
    "# X,X,UI,STY\n",
    "\n",
    "# For convenience, we have included a parsable list of Semantic Types and their abbreviations from the UMLS via the link below. The format of the file is \"Abbreviation|Type Unique Identifier (TUI)|Full Semantic Type Name\". For additional information on Semantic Types, please refer to The UMLS Semantic Network website, the UMLS Semantic Network Fact Sheet, and the Current Semantic Types website .\n",
    "\n",
    "#  Semantic Type Mappings (3.6 kb)\n",
    "\n",
    "# Also, we have included a parsable list of Semantic Groups and their mappings to the Semantic Types via the link below. The format of the file is \"Semantic Group Abbrev|Semantic Group Name|TUI|Full Semantic Type Name\". For additional information on Semantic Groups, please refer to the Semantic Groups web site.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#####\n",
    "# sty_filt\n",
    "\n",
    "SEM_FILT_5 = {'biof': 'Biologic Function',\n",
    "            'blor': 'Body Location or Region',\n",
    "            'celf': 'Cell Function',\n",
    "            'phpr': 'Phenomenon or Process',\n",
    "            'phsf': 'Physiologic Function',\n",
    "           }\n",
    "\n",
    "SEM_FILT_9 = {'biof': 'Biologic Function',\n",
    "            'celf': 'Cell Function',\n",
    "            'fndg': 'Finding',\n",
    "            'moft': 'Molecular Function',\n",
    "            'ortf': 'Organ or Tissue Function',\n",
    "            'orgf': 'Organism Function',\n",
    "            'patf': 'Pathologic Function',\n",
    "            'phpr': 'Phenomenon or Process',\n",
    "            'phsf': 'Physiologic Function',\n",
    "           }\n",
    "\n",
    "SEM_FILT_10 = {\n",
    "    'aapp': 'Amino Acid, Peptide, or Protein',\n",
    "    'biof': 'Biologic Function',\n",
    "            'celf': 'Cell Function',\n",
    "            'fndg': 'Finding',\n",
    "            'moft': 'Molecular Function',\n",
    "            'ortf': 'Organ or Tissue Function',\n",
    "            'orgf': 'Organism Function',\n",
    "            'patf': 'Pathologic Function',\n",
    "            'phpr': 'Phenomenon or Process',\n",
    "            'phsf': 'Physiologic Function',\n",
    "           }\n",
    "\n",
    "sty_filt_lst = [*SEM_FILT_10.values()]\n",
    "\n",
    "sty_filt_tui = sty_df.query('x3 in @sty_filt_lst')['x2'].to_list()\n",
    "#####\n",
    "\n",
    "#####\n",
    "DATA_DIR = './mesh'\n",
    "mesh_desc_path = os.path.join(DATA_DIR, \"d2024.bin\")\n",
    "# mesh_supp_path = os.path.join(DATA_DIR, \"c2017.bin\")\n",
    "\n",
    "mesh_terms = parse_descriptor_records(mesh_desc_path, semantic_types=None)\n",
    "#####\n",
    "\n",
    "#####\n",
    "# mesh_terms\n",
    "# mesh_terms['D000001']['term']\n",
    "ui2term_dct = {k: v['term'] for k, v in mesh_terms.items()}\n",
    "ui2dc_dct = {k: v['descriptor_class'] for k, v in mesh_terms.items()}\n",
    "#####\n",
    "\n",
    "#####\n",
    "ui2tree_dct = {}\n",
    "\n",
    "for k, v in mesh_terms.items():\n",
    "    if 'tree' in v:\n",
    "        ui2tree_dct[k] = v['tree']\n",
    "    else:\n",
    "        ui2tree_dct[k] = None\n",
    "\n",
    "# mesh_terms['D005260']\n",
    "#####\n",
    "\n",
    "#####\n",
    "df_5 = pd.DataFrame.from_dict(ui2term_dct.items())\n",
    "df_5.columns = ['ui', 'term']\n",
    "df_5\n",
    "#####\n",
    "\n",
    "#####\n",
    "df_5['document_class'] = df_5['ui'].map(ui2dc_dct)\n",
    "df_5['tree'] = df_5['ui'].map(ui2tree_dct)\n",
    "#####\n",
    "\n",
    "#####\n",
    "df_6 = df_4.merge(right=df_5, how='left', left_on='mh', right_on='term')\n",
    "df_6 = df_6.filter(['pmid', 'ui', 'term', 'document_class', 'tree'])\n",
    "df_6\n",
    "#####\n",
    "\n",
    "# D008787\tMetoclopramide\n",
    "mesh_terms['D008787']\n",
    "mesh_terms['D008787']['semantic_type']\n",
    "\n",
    "res = []\n",
    "\n",
    "for ui in df_6['ui']:\n",
    "    ui_sty_lst = mesh_terms[ui]['semantic_type']\n",
    "    if (set(ui_sty_lst) & set(sty_filt_tui)):\n",
    "        res.append(ui)\n",
    "\n",
    "ui_sty_lst\n",
    "sty_filt_tui\n",
    "set(['T001', 'T046']) & set(sty_filt_tui)\n",
    "\n",
    "print(f'{len(res)}')\n",
    "print(f'{df_6.shape}')\n",
    "df_6.head()\n",
    "\n",
    "df_6\n",
    "df_7 = df_6.query('ui in @res')\n",
    "print(f'{df_7.shape}')\n",
    "# print(df_7.sort_values('term').to_markdown())\n",
    "\n",
    "print(f'No. of PMIDs: {df_6['pmid'].nunique()}')\n",
    "print(f'No. of MeSH terms: {df_6['term'].size}')\n",
    "\n",
    "print(f'No. of unique UIs: {df_7['ui'].nunique()}')\n",
    "print(f'No. of unique MeSH terms: {df_7['term'].nunique()}')\n",
    "\n",
    "l2l = df_7.groupby('pmid')['term'].apply(list).to_list()\n",
    "\n",
    "# From l2l_sel to TFIDF\n",
    "tf = TfidfVectorizer(tokenizer=lambda x: x, preprocessor=lambda x: x, min_df=3)\n",
    "tf_fit = tf.fit_transform(l2l)\n",
    "wrd_lst = tf.get_feature_names_out()\n",
    "\n",
    "score_lst = np.array(tf_fit.sum(axis=0)).reshape(-1).tolist()\n",
    "wrd2score = dict(zip(wrd_lst, score_lst))\n",
    "\n",
    "df_tf_sel = (pd.DataFrame()\n",
    "             .from_dict(wrd2score, orient='index')\n",
    "             .reset_index()\n",
    "             .set_axis(['name', 'score'], axis=1)\n",
    "             .sort_values(by='score', ascending=False)\n",
    "            )\n",
    "\n",
    "print(df_tf_sel.head(n = 10).to_markdown())\n",
    "# df_tf_sel\n",
    "\n",
    "B -> A: Vasoconstriction\n",
    "df_81 = pd.read_csv('./mig_mg/pmid_dp_ti_mh_vasoconstriction_20241125.psv.gz', sep='|', names=['pmid','dp','ti','mh_lst'])\n",
    "df_82 = pd.DataFrame(df_81['mh_lst'].str.split(';').tolist(), index=df_81.pmid).stack().reset_index([0, 'pmid'])\n",
    "df_82.columns = ['pmid', 'mh']\n",
    "\n",
    "print(f'No. of PubMed records: {df_81.shape}')\n",
    "df_82.head()\n",
    "\n",
    "df_83 = df_82.merge(right=df_5, how='left', left_on='mh', right_on='term')\n",
    "df_83 = df_83.filter(['pmid', 'ui', 'term'])\n",
    "df_83.head()\n",
    "\n",
    "res = []\n",
    "\n",
    "for ui in df_83['ui']:\n",
    "    ui_sty_lst = mesh_terms[ui]['semantic_type']\n",
    "    if (set(ui_sty_lst) & set(['T127', 'T196'])):\n",
    "        res.append(ui)\n",
    "\n",
    "# T127 | Vitamin\n",
    "# T196 | Element, Ion, or Isotope\n",
    "\n",
    "df_84 = df_83.query('ui in @res')\n",
    "# print(df_84.sort_values('term').to_markdown())\n",
    "\n",
    "l2l = df_84.groupby('pmid')['term'].apply(list).to_list()\n",
    "# From l2l_sel to TFIDF\n",
    "tf = TfidfVectorizer(tokenizer=lambda x: x, preprocessor=lambda x: x, min_df=3)\n",
    "tf_fit = tf.fit_transform(l2l)\n",
    "wrd_lst = tf.get_feature_names_out()\n",
    "\n",
    "score_lst = np.array(tf_fit.sum(axis=0)).reshape(-1).tolist()\n",
    "wrd2score = dict(zip(wrd_lst, score_lst))\n",
    "\n",
    "df_tf_sel_1 = (pd.DataFrame()\n",
    "             .from_dict(wrd2score, orient='index')\n",
    "             .reset_index()\n",
    "             .set_axis(['name', 'score'], axis=1)\n",
    "             .sort_values(by='score', ascending=False)\n",
    "            )\n",
    "\n",
    "print(f'No. of unique UIs: {df_84['ui'].nunique()}')\n",
    "print(f'No. of unique MeSH terms: {df_84['term'].nunique()}')\n",
    "\n",
    "df_tf_sel_1.head(n = 10)\n",
    "\n",
    "B -> A: Platelet AggregationÂ¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5979a7-5520-4565-9602-46861ed36acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset for the first domain (migraine literature)\n",
    "# The dataset is stored as pipe-separated value (PSV) compressed files.\n",
    "# Only PMID and MeSH Headings columns are used for subsequent processing.\n",
    "df_3 = pd.read_csv('./pmid_dp_ti_mh_20241124.psv.gz', sep='|', names=['pmid','dp','ti','mh_lst'])\n",
    "df_3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5307efe-69f5-4732-83e6-efd94b7d0aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_4 = pd.DataFrame(df_3['mh_lst'].str.split(';').tolist(), index=df_3.pmid).stack().reset_index([0, 'pmid'])\n",
    "df_4.columns = ['pmid', 'mh']\n",
    "df_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502b2f21-fdd6-4e9a-b721-5a009effb280",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_5 = pd.read_table('../swanson/mrconso_eng_msh_mh.psv.gz', sep='|', header=None, usecols=[*range(0, 18)], nrows=10)\n",
    "\n",
    "# https://github.com/fagan2888/mesh-parser/blob/8bc3f82a6ef600018350222ca3c31007ab238e3a/parser.py#L9\n",
    "\n",
    "# Parse a Descriptor Record\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from itertools import groupby, filterfalse\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "# https://www.nlm.nih.gov/research/umls/knowledge_sources/semantic_network/SemGroups.txt\n",
    "def get_semantic_types(category=None):\n",
    "    # read in semantic types\n",
    "    # st_df = pd.read_csv(\"https://semanticnetwork.nlm.nih.gov/download/SemGroups.txt\", delimiter=\"|\",\n",
    "    #                     names=[\"x0\", \"x1\", \"x2\", \"x3\"])\n",
    "    st_df = pd.read_csv(\"https://www.nlm.nih.gov/research/umls/knowledge_sources/semantic_network/SemGroups.txt\", delimiter=\"|\",\n",
    "                        names=[\"x0\", \"x1\", \"x2\", \"x3\"])\n",
    "    #st_d = dict(zip(st_df['x2'], st_df['x3']))\n",
    "    # cat_df = st_df.query(\"x1 == @category\")[['x2', 'x3']]\n",
    "    # semantic_types = set(cat_df['x2'])\n",
    "    # disorders = {'T037', 'T049', 'T048', 'T050', 'T184', 'T019', 'T190', 'T020', 'T033', 'T046', 'T191', 'T047'}\n",
    "    semantic_types = st_df\n",
    "    return semantic_types\n",
    "\n",
    "def parse_descriptor_records(mesh_desc_path, semantic_types = None):\n",
    "    \"\"\"\n",
    "    semantic_types is optional\n",
    "\n",
    "    :param mesh_desc_path:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    #TODO: only care about disease related attributes. !!!\n",
    "    # for example, ignoring RN CAS REGISTRY/EC NUMBER/UNII CODE  !!!\n",
    "\n",
    "    attributes = {'MH': \"term\",\n",
    "                  'MN': \"tree\",\n",
    "                  'FX': \"see_also\",\n",
    "                  'ST': \"semantic_type\",  # see: https://semanticnetwork.nlm.nih.gov/download/SemGroups.txt\n",
    "                  'MS': \"note\",\n",
    "                  'MR': \"last_updated\",\n",
    "                  'DC': \"descriptor_class\",\n",
    "                  'UI': \"_id\",\n",
    "                  'RECTYPE': \"record_type\",\n",
    "                  'synonyms': \"synonyms\"}  # added by me from PRINT ENTRY & ENTRY\n",
    "\n",
    "    # TODO: parse PRINT ENTRY and ENTRY completely\n",
    "    # \"'D-2-hydroxyglutaric aciduria|T047|EQV|OMIM (2013)|ORD (2010)|090615|abdeef'\"\n",
    "\n",
    "\n",
    "    # read in the mesh data\n",
    "    with open(mesh_desc_path) as f:\n",
    "        mesh_desc = [x.strip() for x in f.readlines()]\n",
    "\n",
    "    # which attributes can have multiple values?\n",
    "    gb = filterfalse(lambda x: x[0], groupby(mesh_desc, lambda x: x == \"*NEWRECORD\"))\n",
    "    ds = []\n",
    "    for gb_record in gb:\n",
    "        record = list(gb_record[1])\n",
    "        d = dict(Counter([line.split(\"=\", 1)[0].strip() for line in record if \"=\" in line]))\n",
    "        ds.append(d)\n",
    "    df = pd.DataFrame(ds).fillna(0)\n",
    "    list_attribs = set(df.columns[df.max() > 1])\n",
    "    # list_attribs = {'EC', 'ENTRY', 'FX', 'MH_TH', 'MN', 'PA', 'PI', 'PRINT ENTRY', 'RR', 'ST'}\n",
    "\n",
    "    # split into records\n",
    "    gb = filterfalse(lambda x: x[0], groupby(mesh_desc, lambda x: x == \"*NEWRECORD\"))\n",
    "\n",
    "    mesh_terms = dict()\n",
    "    for gb_record in gb:\n",
    "        record = list(gb_record[1])\n",
    "        d = defaultdict(list)\n",
    "        for line in record:\n",
    "            if \"=\" not in line:\n",
    "                continue\n",
    "            key = line.split(\"=\", 1)[0].strip()\n",
    "            value = line.split(\"=\", 1)[1].strip()\n",
    "            if key not in attributes and key not in list_attribs:\n",
    "                d[key] = value\n",
    "            elif key in list_attribs and key in attributes:\n",
    "                d[attributes[key]].append(value)\n",
    "            elif key in attributes and key not in {\"PRINT ENTRY\", \"ENTRY\"}:\n",
    "                d[attributes[key]] = value\n",
    "            elif key in {\"PRINT ENTRY\", \"ENTRY\"}:\n",
    "                d['synonyms'].append(value.split(\"|\", 1)[0])\n",
    "\n",
    "        if semantic_types and not (set(d['semantic_type']) & semantic_types):\n",
    "            continue\n",
    "        mesh_terms[d['_id']] = dict(d)\n",
    "\n",
    "    return mesh_terms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bcc59fb-8899-4dc9-ba8c-583a27284a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "sty_df = get_semantic_types()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8a7aff-7fca-4632-8157-cda9eae1ccca",
   "metadata": {},
   "outputs": [],
   "source": [
    "sty_df\n",
    "# print(sty_df.to_markdown())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68ad00f-b102-4cf4-90f3-2d72531cbc54",
   "metadata": {},
   "outputs": [],
   "source": [
    "sty_filt_lst = [*SEM_FILT_10.values()]\n",
    "\n",
    "sty_filt_tui = sty_df.query('x3 in @sty_filt_lst')['x2'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2feb6f53-1bcd-49bf-9658-93dc64ecdd29",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = '../mesh'\n",
    "mesh_desc_path = os.path.join(DATA_DIR, \"d2024.bin\")\n",
    "# mesh_supp_path = os.path.join(DATA_DIR, \"c2017.bin\")\n",
    "\n",
    "mesh_terms = parse_descriptor_records(mesh_desc_path, semantic_types=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692c829c-7205-41f9-85ce-e97134f8e035",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mesh_terms\n",
    "# mesh_terms['D000001']['term']\n",
    "ui2term_dct = {k: v['term'] for k, v in mesh_terms.items()}\n",
    "ui2dc_dct = {k: v['descriptor_class'] for k, v in mesh_terms.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a20a9b-acdb-418c-9f6e-5d440b4ec0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ui2tree_dct = {}\n",
    "\n",
    "for k, v in mesh_terms.items():\n",
    "    if 'tree' in v:\n",
    "        ui2tree_dct[k] = v['tree']\n",
    "    else:\n",
    "        ui2tree_dct[k] = None\n",
    "\n",
    "# mesh_terms['D005260']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb6e27b-4325-4ade-96a3-0cfa2b756351",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_5 = pd.DataFrame.from_dict(ui2term_dct.items())\n",
    "df_5.columns = ['ui', 'term']\n",
    "df_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84ad565-7ba7-484c-98b9-9fecda2def18",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_5['document_class'] = df_5['ui'].map(ui2dc_dct)\n",
    "df_5['tree'] = df_5['ui'].map(ui2tree_dct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372c17c7-74b8-4337-889c-6c05dd9480f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52176b29-75a5-48a9-b669-5e9773a15e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_6 = df_4.merge(right=df_5, how='left', left_on='mh', right_on='term')\n",
    "df_6 = df_6.filter(['pmid', 'ui', 'term', 'document_class', 'tree'])\n",
    "df_6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da716137-411b-4fbf-b3dd-969ecbc50b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# D008787\tMetoclopramide\n",
    "mesh_terms['D008787']\n",
    "mesh_terms['D008787']['semantic_type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93e571c-c8cb-4860-b749-9b30e645fb09",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = []\n",
    "\n",
    "for ui in df_6['ui']:\n",
    "    ui_sty_lst = mesh_terms[ui]['semantic_type']\n",
    "    if (set(ui_sty_lst) & set(sty_filt_tui)):\n",
    "        res.append(ui)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b4bdc9-2bd7-4c02-adb0-e4d0fcfd1844",
   "metadata": {},
   "outputs": [],
   "source": [
    "ui_sty_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56351385-43f3-43ae-9af3-5d2363831f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sty_filt_tui\n",
    "set(['T001', 'T046']) & set(sty_filt_tui)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21be3c93-206f-4303-baa7-54eea2456a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(res)\n",
    "df_6.shape\n",
    "df_6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ca0a95-de3b-4292-9947-6d61d7911115",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_6\n",
    "df_7 = df_6.query('ui in @res')\n",
    "print(f'{df_7.shape}')\n",
    "# print(df_7.sort_values('term').to_markdown())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02396205-bb5f-4e0e-9645-3e872c77266c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'No. of PMIDs: {df_6['pmid'].nunique()}')\n",
    "print(f'No. of MeSH terms: {df_6['term'].size}')\n",
    "\n",
    "print(f'No. of unique UIs: {df_7['ui'].nunique()}')\n",
    "print(f'No. of unique MeSH terms: {df_7['term'].nunique()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47831ebd-2201-4a68-9c9f-7dbfba6448d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "l2l = df_7.groupby('pmid')['term'].apply(list).to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8557d9-9945-4e27-8b32-d0c7526aa88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From l2l_sel to TFIDF\n",
    "tf = TfidfVectorizer(tokenizer=lambda x: x, preprocessor=lambda x: x, min_df=3)\n",
    "tf_fit = tf.fit_transform(l2l)\n",
    "wrd_lst = tf.get_feature_names_out()\n",
    "\n",
    "score_lst = np.array(tf_fit.sum(axis=0)).reshape(-1).tolist()\n",
    "wrd2score = dict(zip(wrd_lst, score_lst))\n",
    "\n",
    "df_tf_sel = (pd.DataFrame()\n",
    "             .from_dict(wrd2score, orient='index')\n",
    "             .reset_index()\n",
    "             .set_axis(['name', 'score'], axis=1)\n",
    "             .sort_values(by='score', ascending=False)\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce19c86-4828-4a53-a9bd-6d74f0a1edf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_tf_sel.to_markdown())\n",
    "# df_tf_sel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46c6a24-fc0c-4a6a-b705-a0d745847223",
   "metadata": {},
   "source": [
    "## B -> A :: Vasoconstriction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43c67d4-d79e-4dec-ad67-01694392fcd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_81 = pd.read_csv('./pmid_dp_ti_mh_vasoconstriction_20241125.psv.gz', sep='|', names=['pmid','dp','ti','mh_lst'])\n",
    "df_82 = pd.DataFrame(df_81['mh_lst'].str.split(';').tolist(), index=df_81.pmid).stack().reset_index([0, 'pmid'])\n",
    "df_82.columns = ['pmid', 'mh']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36620f1b-be56-40d2-b4f9-5fdd3942e644",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'No. of PubMed records: {df_81.shape}')\n",
    "df_82"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82fe42b-3fb2-481b-ac91-175969221aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_83 = df_82.merge(right=df_5, how='left', left_on='mh', right_on='term')\n",
    "df_83 = df_83.filter(['pmid', 'ui', 'term'])\n",
    "df_83"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea807b9-65e7-4f3f-9823-98abf0a1a1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = []\n",
    "\n",
    "for ui in df_83['ui']:\n",
    "    ui_sty_lst = mesh_terms[ui]['semantic_type']\n",
    "    if (set(ui_sty_lst) & set(['T127', 'T196'])):\n",
    "        res.append(ui)\n",
    "\n",
    "# T127 | Vitamin\n",
    "# T196 | Element, Ion, or Isotope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0051f6-579a-4ad1-b94f-2b80e84ea54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_84 = df_83.query('ui in @res')\n",
    "# print(df_84.sort_values('term').to_markdown())\n",
    "\n",
    "l2l = df_84.groupby('pmid')['term'].apply(list).to_list()\n",
    "# From l2l_sel to TFIDF\n",
    "tf = TfidfVectorizer(tokenizer=lambda x: x, preprocessor=lambda x: x, min_df=3)\n",
    "tf_fit = tf.fit_transform(l2l)\n",
    "wrd_lst = tf.get_feature_names_out()\n",
    "\n",
    "score_lst = np.array(tf_fit.sum(axis=0)).reshape(-1).tolist()\n",
    "wrd2score = dict(zip(wrd_lst, score_lst))\n",
    "\n",
    "df_tf_sel_1 = (pd.DataFrame()\n",
    "             .from_dict(wrd2score, orient='index')\n",
    "             .reset_index()\n",
    "             .set_axis(['name', 'score'], axis=1)\n",
    "             .sort_values(by='score', ascending=False)\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69f9de0-803a-4f84-87a9-2d3e9b30e530",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'No. of unique UIs: {df_84['ui'].nunique()}')\n",
    "print(f'No. of unique MeSH terms: {df_84['term'].nunique()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb7287b-c792-4393-9d51-b224421758df",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tf_sel_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8281e314-302f-4570-b9fd-fa8dcf03e5a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dddd13a4-671a-4726-a871-8fd30f32d749",
   "metadata": {},
   "source": [
    "## B -> A :: Platelet Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3159b06-015a-4eaf-a40f-3cf4eeb2a106",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_91 = pd.read_csv('./pmid_dp_ti_mh_platelet_20241125.psv.gz', sep='|', names=['pmid','dp','ti','mh_lst'])\n",
    "df_92 = pd.DataFrame(df_91['mh_lst'].str.split(';').tolist(), index=df_91.pmid).stack().reset_index([0, 'pmid'])\n",
    "df_92.columns = ['pmid', 'mh']\n",
    "\n",
    "df_93 = df_92.merge(right=df_5, how='left', left_on='mh', right_on='term')\n",
    "df_93 = df_93.filter(['pmid', 'ui', 'term'])\n",
    "\n",
    "res = []\n",
    "\n",
    "for ui in df_93['ui']:\n",
    "    ui_sty_lst = mesh_terms[ui]['semantic_type']\n",
    "    if (set(ui_sty_lst) & set(['T127', 'T196'])):\n",
    "        res.append(ui)\n",
    "\n",
    "df_94 = df_93.query('ui in @res')\n",
    "\n",
    "l2l = df_94.groupby('pmid')['term'].apply(list).to_list()\n",
    "# From l2l_sel to TFIDF\n",
    "tf = TfidfVectorizer(tokenizer=lambda x: x, preprocessor=lambda x: x, min_df=3)\n",
    "tf_fit = tf.fit_transform(l2l)\n",
    "wrd_lst = tf.get_feature_names_out()\n",
    "\n",
    "score_lst = np.array(tf_fit.sum(axis=0)).reshape(-1).tolist()\n",
    "wrd2score = dict(zip(wrd_lst, score_lst))\n",
    "\n",
    "df_tf_sel_2 = (pd.DataFrame()\n",
    "             .from_dict(wrd2score, orient='index')\n",
    "             .reset_index()\n",
    "             .set_axis(['name', 'score'], axis=1)\n",
    "             .sort_values(by='score', ascending=False)\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d424622-cf6c-408e-992b-2cc69478575a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'No. of PubMed records: {df_91.shape}')\n",
    "print(f'No. of unique UIs: {df_94['ui'].nunique()}')\n",
    "print(f'No. of unique MeSH terms: {df_94['term'].nunique()}')\n",
    "\n",
    "# No. of PubMed records: (6273, 4)\n",
    "# No. of unique UIs: 73\n",
    "# No. of unique MeSH terms: 73"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e587f9e-c261-490b-984a-22162a90e6bf",
   "metadata": {},
   "source": [
    "## B -> A :: Spreading Cortical Depression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4412a35a-b8f5-4105-8a45-05e7af1a118f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_101 = pd.read_csv('./pmid_dp_ti_mh_cortical_20241125.psv.gz', sep='|', names=['pmid','dp','ti','mh_lst'])\n",
    "df_102 = pd.DataFrame(df_101['mh_lst'].str.split(';').tolist(), index=df_101.pmid).stack().reset_index([0, 'pmid'])\n",
    "df_102.columns = ['pmid', 'mh']\n",
    "\n",
    "df_103 = df_102.merge(right=df_5, how='left', left_on='mh', right_on='term')\n",
    "df_103 = df_103.filter(['pmid', 'ui', 'term'])\n",
    "\n",
    "res = []\n",
    "\n",
    "for ui in df_103['ui']:\n",
    "    ui_sty_lst = mesh_terms[ui]['semantic_type']\n",
    "    if (set(ui_sty_lst) & set(['T127', 'T196'])):\n",
    "        res.append(ui)\n",
    "\n",
    "df_104 = df_103.query('ui in @res')\n",
    "\n",
    "l2l = df_104.groupby('pmid')['term'].apply(list).to_list()\n",
    "# From l2l_sel to TFIDF\n",
    "tf = TfidfVectorizer(tokenizer=lambda x: x, preprocessor=lambda x: x, min_df=3)\n",
    "tf_fit = tf.fit_transform(l2l)\n",
    "wrd_lst = tf.get_feature_names_out()\n",
    "\n",
    "score_lst = np.array(tf_fit.sum(axis=0)).reshape(-1).tolist()\n",
    "wrd2score = dict(zip(wrd_lst, score_lst))\n",
    "\n",
    "df_tf_sel_3 = (pd.DataFrame()\n",
    "             .from_dict(wrd2score, orient='index')\n",
    "             .reset_index()\n",
    "             .set_axis(['name', 'score'], axis=1)\n",
    "             .sort_values(by='score', ascending=False)\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c5b0fe-c8ad-478f-a4e5-0ce2df400614",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'No. of PubMed records: {df_101.shape}')\n",
    "print(f'No. of unique UIs: {df_104['ui'].nunique()}')\n",
    "print(f'No. of unique MeSH terms: {df_104['term'].nunique()}')\n",
    "\n",
    "# No. of PubMed records: (180, 4)\n",
    "# No. of unique UIs: 13\n",
    "# No. of unique MeSH terms: 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7528f814-a49f-4e2c-9983-1678cffbef39",
   "metadata": {},
   "outputs": [],
   "source": [
    "set(df_84['term'].to_list()) & set(df_94['term'].to_list()) & set(df_104['term'].to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6cdce8-63eb-4a17-a2b9-ff7ca08ca05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Cortical Spreading Depression[MH] AND 1966/01/01:1987/12/31[DP] AND medline[sb] AND hasabstract AND english[LA]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d3fdd3-20e2-4893-ad02-f4fa966271b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee316be-1d17-44d0-b5e2-f3c50ea18a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common MeSH headings across all three B terms\n",
    "set(df_84['term'].to_list()) & set(df_94['term'].to_list()) & set(df_104['term'].to_list())\n",
    "\n",
    "Cortical Spreading Depression[MH] AND 1966/01/01:1987/12/31[DP] AND medline[sb] AND hasabstract AND english[LA]\n",
    "\n",
    "'Calcium' 4\n",
    "'Iodine Radioisotopes' 1\n",
    "'Lithium' 5\n",
    "'Magnesium' 0\n",
    "'Manganese' 0\n",
    "'Oxygen' 6\n",
    "'Potassium' 4\n",
    "'Sodium' 2\n",
    "'Tritium' 0\n",
    "'Xenon Radioisotopes' 10\n",
    "\n",
    "tmp = pd.DataFrame({'s1': df_tf_sel_1['name'],\n",
    "              's2': df_tf_sel_2['name'],\n",
    "              's3': df_tf_sel_3['name']})\n",
    "\n",
    "tmp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07d2762-94de-498d-bedc-7929531cdb09",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Robust Rank Aggregation (Kolde et al., 2012)\n",
    "# Partially based on R implementation\n",
    "# TODO Drop pandas\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from scipy.stats import beta\n",
    "\n",
    "def rank_matrix(glist, N=None, full=False):\n",
    "    # Get the unique elements from the lists in glist\n",
    "    u = pd.Index(set().union(*glist))\n",
    "    \n",
    "    # If N is None or empty, set it to the length of unique elements\n",
    "    if N is None:\n",
    "        N = len(u)\n",
    "    \n",
    "    if not full:\n",
    "        # Initialize a matrix with 1s\n",
    "        rmat = pd.DataFrame(\n",
    "            np.ones((len(u), len(glist))),\n",
    "            index=u,\n",
    "            columns=range(len(glist))\n",
    "        )\n",
    "        # Expand N to match the number of columns in the matrix\n",
    "        if isinstance(N, int):\n",
    "            N = [N] * rmat.shape[1]\n",
    "    else:\n",
    "        # Initialize a matrix with NaNs\n",
    "        rmat = pd.DataFrame(\n",
    "            np.full((len(u), len(glist)), np.nan),\n",
    "            index=u,\n",
    "            columns=range(len(glist))\n",
    "        )\n",
    "        # Set N as the lengths of the individual lists in glist\n",
    "        N = [len(g) for g in glist]\n",
    "    \n",
    "    # Fill in the rank values for each list in glist\n",
    "    for i, g in enumerate(glist):\n",
    "        rmat.loc[g, i] = [(j + 1) / N[i] for j in range(len(g))]\n",
    "    \n",
    "    return rmat\n",
    "\n",
    "\n",
    "def beta_scores(r):\n",
    "    # Count non-NA values\n",
    "    n = np.sum(~np.isnan(r))\n",
    "    \n",
    "    # Create an array of ones of length n\n",
    "    p = np.ones(n)\n",
    "    \n",
    "    # Sort r, placing NaNs at the end\n",
    "    r = np.sort(r[~np.isnan(r)])\n",
    "    \n",
    "    # Compute the beta probabilities\n",
    "    p = beta.cdf(r, a=np.arange(1, n + 1), b=n - np.arange(1, n + 1) + 1)\n",
    "    \n",
    "    return p\n",
    "\n",
    "def correct_beta_pvalues(p, k):\n",
    "    # Adjust the p-values and ensure they don't exceed 1\n",
    "    p = min(p * k, 1)\n",
    "    \n",
    "    return p\n",
    "\n",
    "def rho_scores(r):\n",
    "    # Compute beta scores\n",
    "    x = beta_scores(r)\n",
    "    \n",
    "    # Compute rho value\n",
    "    rho = correct_beta_pvalues(np.nanmin(x), k=np.sum(~np.isnan(x)))\n",
    "    \n",
    "    return rho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5905dd2-8ec6-4535-8224-7e540001c962",
   "metadata": {},
   "outputs": [],
   "source": [
    "g_lst = [df_tf_sel_1['name'].to_list(),\n",
    "         df_tf_sel_2['name'].to_list(),\n",
    "         df_tf_sel_3['name'].to_list()]\n",
    "\n",
    "rm_df = rank_matrix(g_lst)\n",
    "# rm_df.transform(np.sort)\n",
    "\n",
    "rm_df.apply(rho_scores, axis=1).sort_values()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
